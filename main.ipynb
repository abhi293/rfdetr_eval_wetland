{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f16b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRBase\n",
    "\n",
    "model = RFDETRBase()\n",
    "\n",
    "# Path to your checkpoint\n",
    "checkpoint_path = \"rfdetr_results/checkpoint_best_ema.pth\"\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=\"dataset\",\n",
    "    output_dir=\"rfdetr_results\",\n",
    "    \n",
    "    # Continuation Logic\n",
    "    resume=checkpoint_path,\n",
    "    start_epoch=3,            \n",
    "    epochs=15,                # Reduced to 15 to stay within your 4hr window\n",
    "    \n",
    "    # Corrected Settings\n",
    "    resolution=448,           \n",
    "    multi_scale=True,         \n",
    "    expanded_scales=True,     # Fix: Change list to True\n",
    "    \n",
    "    # Memory Management\n",
    "    batch_size=4,             \n",
    "    grad_accum_steps=4,       \n",
    "    amp=True,                 \n",
    "    num_workers=2,            \n",
    "    checkpoint_interval=1     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rfdetr import RFDETRBase\n",
    "\n",
    "model = RFDETRBase()\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=\"dataset\",\n",
    "    epochs=5,               # Set to 10; it will save progress even if it stops early\n",
    "    output_dir=\"/content/rfdetr_results\",\n",
    "    batch_size=4,            # Processes 4 images at once (uses more VRAM, faster)\n",
    "    grad_accum_steps=2,      # Only 2 steps to reach the target batch of 16\n",
    "    checkpoint_interval=1    # Save every single epoch so no work is lost!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRBase\n",
    "\n",
    "model = RFDETRBase()\n",
    "\n",
    "# Use the Best EMA checkpoint for maximum accuracy\n",
    "checkpoint_path = \"rfdetr_results/checkpoint_best_ema.pth\"\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=\"dataset\",\n",
    "    output_dir=\"rfdetr_results\",\n",
    "    \n",
    "    # Continuation Logic\n",
    "    resume=checkpoint_path,\n",
    "    start_epoch=3,            # Tells the logger to start counting from 3\n",
    "    epochs=15,                \n",
    "    \n",
    "    # HIGH QUALITY SETTINGS (Restored)\n",
    "    # Removing 'resolution' forces it back to the original high-quality default\n",
    "    multi_scale=True,         \n",
    "    expanded_scales=False,    # Keep this False to prevent hallucinations\n",
    "    \n",
    "    # MEMORY PROTECTION \n",
    "    # High resolution takes more VRAM. We use a smaller batch but more accumulation.\n",
    "    batch_size=2,             # Only 2 images in memory at once\n",
    "    grad_accum_steps=8,       # 2 x 8 = 16 (matches your successful original batch size)\n",
    "    amp=True,                 \n",
    "    num_workers=2,            \n",
    "    checkpoint_interval=1     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e11016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRBase\n",
    "\n",
    "model = RFDETRBase()\n",
    "\n",
    "# Use the Best EMA checkpoint for maximum accuracy\n",
    "checkpoint_path = \"rfdetr_results/checkpoint_best_ema.pth\"\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=\"dataset\",\n",
    "    output_dir=\"rfdetr_results\",\n",
    "    \n",
    "    # Continuation Logic\n",
    "    resume=checkpoint_path,\n",
    "    num_classes=14,  #or maybe 13\n",
    "    start_epoch=3,            # Tells the logger to start counting from 3\n",
    "    epochs=4,                \n",
    "    # 2. DISABLE the things causing the jump in cardinality\n",
    "    multi_scale=False,       # Force a single scale to stabilize the head\n",
    "    expanded_scales=False,   # Turn off expanded scales (the 840px scale is too much right now)\n",
    "    # MEMORY PROTECTION \n",
    "    # High resolution takes more VRAM. We use a smaller batch but more accumulation.\n",
    "    batch_size=4,             # Only 2 images in memory at once\n",
    "    grad_accum_steps=2,       # 2 x 4 = 8 (matches your successful original batch size)\n",
    "    amp=True,                             \n",
    "    checkpoint_interval=1     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rfdetr import RFDETRBase\n",
    "import os\n",
    "\n",
    "# 1. Initialize fresh model\n",
    "model = RFDETRBase()\n",
    "\n",
    "# 2. Point to your best weights\n",
    "checkpoint_path = \"rfdetr_results/checkpoint_best_ema.pth\"\n",
    "\n",
    "# 3. FORCE a fresh folder to prevent any hidden config issues\n",
    "# We will use 'rfdetr_finetune' instead of 'rfdetr_results'\n",
    "new_output_dir = \"rfdetr_finetune\"\n",
    "\n",
    "model.train(\n",
    "    dataset_dir=\"dataset\",\n",
    "    output_dir=new_output_dir, \n",
    "    \n",
    "    # IMPORTANT: Use 'pretrain_weights' instead of 'resume'\n",
    "    # This loads the patterns/learning but RESETs the learning rate and optimizer\n",
    "    pretrain_weights=checkpoint_path,\n",
    "    resume=None,               \n",
    "    \n",
    "    num_classes=14,\n",
    "    start_epoch=0,           # Reset clock to 0 so the LR starts fresh\n",
    "    epochs=2,               # Give it more time to stabilize\n",
    "    \n",
    "    # CRITICAL: Ultra-low LR to let the new head stabilize\n",
    "    lr=0.000005,             # 5e-6 (even smaller than before)\n",
    "    warmup_epochs=1.0,       # 1 full epoch of slow-start\n",
    "    \n",
    "    multi_scale=False,\n",
    "    expanded_scales=False,\n",
    "    batch_size=4,            # Smaller batch + more accumulation = more stability\n",
    "    grad_accum_steps=2,      # Effective batch of 8\n",
    "    amp=True,\n",
    "    num_workers=2,\n",
    "    checkpoint_interval=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
